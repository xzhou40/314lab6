{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Simple Twitter Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will write a simple python scripts to collect data from Twitter using a python wrapper of Twitter API - tweepy. We will then fit the InterTweet Interval (ITI) into a power distribution, to check whether or not it follows a powerlaw (for reference of Powerlaw: http://tuvalu.santafe.edu/~aaronc/powerlaws/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What to hand in: \n",
    "You will need to pack following things into a file.\n",
    "\n",
    "\n",
    "   * The completed Notebook file (ipynb) - Remember to answer all the questions in the notebook!\n",
    "   * Both the data files you collect (*.json, with 100 and 3,000 tweets)\n",
    "   * All the figures plotted in this lab (Tweet distribution, inter-tweet distribution and power law fit for every run (more if you are going for extra credit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Collecting Twitter Data Using Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first exercise, you will be required to collect a certain number of Tweets by searching a hashtag or sent within a bounding box using Twitter Stream API, and store all the collected tweets into a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction to the Twitter Stream API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter provides two kinds of API for developers, the REST APIs and Streaming APIs. The REST APIs provides programmatic access to read and write Twitter data. While the Streaming APIs continuously deliver new responses to REST API queries over a long-lived HTTP connection. It receives updates on the latest Tweets matching a search query, stay in sync with user profile updates, and more. In this lab, since the target is to study the inter-tweet disctibution, we need to collect real-time tweets. Thus, we will use the Stream API based on a Python implementation \"tweepy\". For more information about Twitter API, please read the information on https://dev.twitter.com/overview/documentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Register: Twitter Application Managment https://apps.twitter.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Twitter APIs require to authorized request to Twitter via an OAuth endpoints. To start,\n",
    "- Login to your Twitter account\n",
    "- Go to https://apps.twitter.com/app/ \n",
    "- Create a new app, enter a valid app name and URL\n",
    "- Once done, you will go to another page.\n",
    "- First switch to the permission tab and change the permissions to read write and direct messages.\n",
    "- Then switch to the Keys and Access Tokens tab and generate the consumer key and secret, as well as the access token and secret (By selecting 'Create my access token' if needed).\n",
    "\n",
    "You are now ready to proceed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using and Understanding JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Twitter API search results are returned in a JSON format. JSON is a way to encode complicated information in a platform-independent way. It could be considered the lingua franca of information exchange on the Internet. It is a rather simplistic and elegant way to encode complex data structures. To encode and decode JSON, we need the package \"json\". More about JSON: https://docs.python.org/2/library/json.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will start a Twitter Stream API implemented by the Python package \"tweepy\" to collect a certain number of tweets of a specific location. (pip install tweepy if needed)\n",
    "\n",
    "Using the following python code collect 100 tweets of a location of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, time, sys, tweepy\n",
    "from IPython.display import clear_output\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go to http://dev.twitter.com and create an app.\n",
    "# The consumer key and secret will be generated for you after\n",
    "consumer_key=\"\"\n",
    "consumer_secret=\"\n",
    "\n",
    "# After the step above, you will be redirected to your app's page.\n",
    "# Create an access token under the the \"Your access token\" section\n",
    "access_token=\"\n",
    "access_token_secret=\"\"\n",
    "\n",
    "# List of cities: the numbers are the centers of each city, with square -/+ 0.2, using their latitudes and\n",
    "# longitudes. Feel free to add your hometown and analyze if you want!\n",
    "\n",
    "location_list = {'New York City, New York' : [-74.00 - 0.2, 40.71 - 0.2, -74.00 + 0.2, 40.71 + 0.2],\n",
    "                 'Chicago, Illinois' : [-87.63 - 0.2, 41.88 - 0.2, -87.63 + 0.2, 41.88 + 0.2],\n",
    "                 'Bloomington, Indiana' : [-86.53 - 0.2, 39.16 - 0.2, -86.53 + 0.2, 39.16 + 0.2], \n",
    "                 'Iowa City, Iowa' : [-91.53 - 0.2, 41.47 - 0.2, -91.53 + 0.2, 41.47 + 0.2], \n",
    "                 'Ann Arbor, Michigan' : [-83.75 - 0.2, 42.28 - 0.2, -83.75 + 0.2, 42.28 + 0.2],\n",
    "                 'East Lansing, Michigan' :[-84.48 - 0.2, 42.73 - 0.2, -84.48 + 0.2, 42.73 + 0.2],\n",
    "                 'Lincoln, Nebraska' : [-96.68 - 0.2, 40.81 - 0.2, -96.68 + 0.2, 40.81 + 0.2], \n",
    "                 'Columbus, Ohio' : [-82.98 - 0.2, 39.98 - 0.2, -82.98 + 0.2, 39.98 + 0.2], \n",
    "                 'State College, Pennsylvania' : [-77.86 - 0.2, 40.79 - 0.2, -77.86 + 0.2, 40.79 + 0.2], \n",
    "                 'West Lafayette, Indiana' : [-86.91 - 0.2, 40.44 - 0.2, -86.91 + 0.2, 40.44 + 0.2], \n",
    "                 'Madison, Wisconsin' : [-89.4 - 0.2, 43.07 - 0.2, -89.4 + 0.2, 43.07 + 0.2], \n",
    "                 'Champaign, Illinois' : [-88.27 - 0.2, 40.12 - 0.2, -88.27 + 0.2, 40.12 + 0.2]\n",
    "                }\n",
    "\n",
    "# The JSON listener class.\n",
    "\n",
    "class JSONListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets are the received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, fprefix = 'streamer', max_tweets = 100):\n",
    "        self.counter = 0\n",
    "        self.fprefix = fprefix\n",
    "        self.output  = open('./data/' + fprefix + '.json', 'w')\n",
    "        self.max_tweets = max_tweets\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        if self.counter < self.max_tweets:\n",
    "            self.output.write(data)\n",
    "            self.counter += 1\n",
    "          \n",
    "            clear_output()\n",
    "            print 'Tweets collected so far: {}'.format(self.counter)\n",
    "            # Print the information\n",
    "            # Comment these lines of code when you are crawling data for assignment 3!!!\n",
    "            #'''\n",
    "        #    status = json.loads(data)\n",
    "        #    print 'Tweet No. {} at [Time = {}]'.format(self.counter, status['created_at'])\n",
    "        #    print status['text']\n",
    "            #'''\n",
    "            return True\n",
    "        else:\n",
    "            print 'Totally {} tweets collected'.format(self.max_tweets)\n",
    "            return False\n",
    "        \n",
    "\n",
    "    def on_error(self, status):\n",
    "        print 'Error: ' + str(status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 1. COMPLETE CODE BELOW ##########################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "    listen = JSONListener(fprefix = __________, max_tweets = ____________)\n",
    "    city = location_list[______________]\n",
    "    \n",
    "    stream = Stream(auth, listen)\n",
    "    stream.filter(locations=city)\n",
    "    \n",
    "    print 'Job completed. Disconnecting stream...'\n",
    "    stream.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Tweets Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 2, you will be required to fit a power distribution of the InterTweet Interval based on \n",
    "\n",
    "   1. The data collected in Exercise 1 for a location of your choice (100 tweets)\n",
    "   2. A given dataset in ./data/CU.json (This corresponds to 1000 tweets from Champaign) \n",
    "\n",
    "To facillitate the powerlaw calculation, a Python package \"powerlaw\" will be used.\n",
    "\n",
    "### Tips for Exercise 2: \n",
    "* Please don't leave figures in the notebook file, otherwise the file will be extremely large to open. \n",
    "* Use plt.savefig(FILENAME) to save figure into disk directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Simple Analytics of ITI (InterTweet Interval) based on the data collected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Data preprocessing:\n",
    "\n",
    "Import JSON file, and read the information to memory. For this, use ['timestamp_ms'] which is measured in ms. You need to first convert the time into seconds and plot the time series distribution. Using plt.hist() and plt.show() functions to plot the distribution. And the number of bins could be set as the count of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#store the time series in an array/list, named 'data'\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, json\n",
    "import numpy as np\n",
    "#Don't use inline option if you don't want to show figure in notebook.\n",
    "#%matplotlib inline\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 2. COMPLETE CODE BELOW ##########################\n",
    "\n",
    "fp = open('./data/NY.json','r')\n",
    "for line in fp:\n",
    "    _________________________________\n",
    "    _________________________________\n",
    "\n",
    "\n",
    "plt.hist(data, bins = _____)\n",
    "plt.title('Time Series Distribution')\n",
    "plt.savefig('.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Inter-Tweet Intervals (ITI). Note, by turning the time from ms to sec, the time series turns to be a float number series, which is a continuous distribution.  Sort the data. Then calculate inter-tweet intervals.\n",
    "\n",
    "Hints: \n",
    "1. You may find the functions np.sort() and np.diff() useful\n",
    "2. Before using plt.hist() to show your distribution, using np.sort() to sort the ITI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 3. WRITE YOUR CODE BELOW ##########################\n",
    "import numpy as np\n",
    "data = \n",
    "x = \n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Fit the Distribution of ITI using Power-Law. \n",
    "\n",
    "A continuous power-law distribution is one described by a probability density function $p(x)$ such that $p(x)=C f(x)$, where $C$ is a normalization constant, $X$ is the observd value, and $f(x) = x^{-\\alpha}$.\n",
    "\n",
    "Typically, the explicit form of a power-law distribution is:\n",
    "\\begin{equation} p(x) = \\frac{\\alpha-1}{x_{min}} (\\frac{x}{x_{min}})^{-\\alpha} \\end{equation}\n",
    "where $x_{min}$ is the lower bound to the power-law distribution such that it holds $\\forall x \\geq x_{min}$ with $\\alpha \\geq 1$.\n",
    "\n",
    "Sometimes, it is much easier to consider the (complementary) cumulative distribution function (CCDF) of a power-law distributed variable, which is denoted as $\\bar{P}(x)=Pr(X\\geq x)$. In the continuous case, \n",
    "\\begin{equation}\n",
    "\\bar{P}(x)=\\int_x p(x')dx' = (\\frac{x}{x_{min}})^{-\\alpha + 1}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the cumulative distribution function (CDF) is then simply \n",
    "\\begin{equation}\n",
    "P(x) = 1 - \\bar{P}(x)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1: To estimate the parameters in the power-law, you need to implement a function to calculate the estimator of $\\alpha$, in the continuous case, as \n",
    "\\begin{equation}\n",
    "\\hat{\\alpha}=1+n(\\sum_{i=1}^n \\ln \\frac{x_i}{x_{min}})^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x_i \\geq x_{min}, n = \\text{len}(x_i>=x_{min})\n",
    "\\end{equation}\n",
    "\n",
    " \n",
    "\n",
    "In this implementation, you need to assume the parameter $x_{min}$ is given or by default is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of estimator of alpha for power-law\n",
    "import numpy \n",
    "import time\n",
    "import pylab\n",
    "from numpy import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 4. WRITE YOUR CODE BELOW ##########################\n",
    "\n",
    "def alpha(x, xmin = 0):\n",
    "    # Your implementation here. \n",
    "    # Make sure to add a small value to the denominator of the log so that the term does\n",
    "    # not blow up for small values of x_min\n",
    "    \n",
    "       \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2: Estimating the lower bound on power-law behavior, $x_{min}$\n",
    "\n",
    "The power-law fitting relies on the estimation of $x_{min}$. An efficient algorithm to estimate the lower bound is proposed by Clauset et al., and the fundamental idea behind it is: we choose the value of $\\hat{x}_{min}$ that makes the probability distribution of the measured data and the best-fit power-law model as similar as possible above $x_{min}$ (i.e., $\\forall x \\geq x_{min}$).\n",
    "\n",
    "To measure the distance between two probability distributions, the commonest distance used is Kolmogorov-Smirnov or KS statistic, which is simply the maximum distance between the CDFs of the data and the fitted model:\n",
    "\\begin{equation}\n",
    "D=\\max_{x\\geq x_{min}} |S(x)-P(x)|\n",
    "\\end{equation}\n",
    "and the best $x_{min}$ is achieved by\n",
    "\\begin{equation}\n",
    "\\hat{x}_{min} = \\arg\\min_{x_{min} \\in X} D_{x_{min}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a function to calculate the KS statistic is given, as kstest(xmin, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kstest(xmin,x):\n",
    "    x = x[x>=xmin]\n",
    "    n = len(x)\n",
    "    if n == 0: return numpy.inf\n",
    "    # calculate the best power-law fit\n",
    "    a = alpha(x, xmin)\n",
    "    # Actual CDF\n",
    "    cx = numpy.arange(n,dtype='float')/float(n)\n",
    "    # CDF of the fitted power-law distribution\n",
    "    cf = 1-(xmin/x)**(a-1)\n",
    "    ks = max(abs(cf-cx))\n",
    "    return ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-3: Use the above functions, to implement the power-law fitting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 5. WRITE YOUR CODE BELOW ##########################\n",
    "\n",
    "def plfit(x):\n",
    "    # Your implementation here, ensure x is sorted\n",
    "    '''\n",
    "    Tips: choose x_min candidate from\n",
    "    xmins,argxmins = numpy.unique(x,return_index=True)\n",
    "    search the candidate xmins to determine the best xmin and alpha\n",
    "    and use the function numpy.argmin() to select minimal value\n",
    "    '''\n",
    "   \n",
    "    # The calculation of Likelihood is: L = n*log((alpha-1)/xmin) - alpha*sum(log(z/xmin)), with z = x[x>xmin]\n",
    "    \n",
    "    # Return value is (xmin, a, ks, L)\n",
    "    # a - alpha, ks - KS distance, L - likelihood\n",
    "    return (xmin, a, ks, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test the Goodness of PowerLaw fitting\n",
    "\n",
    "Given an observed data set and a hypothesized power-law distribution from which the data are drawn, we would like to know whether the hypothesis is a plausible one, given the data. A standard approach is to use a goodness-of-fit test, which generate a $p$-value that quantifies the plausibility of the hypothesis. \n",
    "\n",
    "The basic idea of such test is based on measurement of the \"distance\" between the distribution of empirical data and the hypothesized model. This distance is compared with distance measurements for comparable synthetic data sets drawn from the same model, and the p-value is defined to be the fraction of the synthetic distances that are larger than the empirical distance.\n",
    "\n",
    "If $p$ is large (close to 1), then the difference between the empirical data and the model can be attributed to the statistical fluctuations alone; if it is small, the model is not a plausible fit to the data. Typically, we use $0.1$ as threshold, that is if $p \\geq 0.1$, the model fitting is plausible, otherwise not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following is a function to test the $p$-value, use the function to test your power-law fitting, where xmin, alpha, ks is the returned value of plfit for the empirical data. And returned value is p-value and the ks vector for the random generated synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy.random as npr\n",
    "\n",
    "def test_pl(x, xmin, alpha, ks, niter=1e2, print_timing=False):\n",
    "    \"\"\"\n",
    "    Monte-Carlo test to determine whether distribution is consistent with a power law\n",
    "\n",
    "    Runs through niter iterations of a sample size identical to the input sample size.\n",
    "\n",
    "    Will randomly select values from the data < xmin.  The number of values selected will\n",
    "    be chosen from a uniform random distribution with p(<xmin) = n(<xmin)/n.\n",
    "\n",
    "    Once the sample is created, it is fit using above methods, then the best fit is used to\n",
    "    compute a Kolmogorov-Smirnov statistic.  The KS stat distribution is compared to the \n",
    "    KS value for the fit to the actual data, and p = fraction of random ks values greater\n",
    "    than the data ks value is computed.  If p<.1, the data may be inconsistent with a \n",
    "    powerlaw.  A data set of n(>xmin)>100 is required to distinguish a PL from an exponential,\n",
    "    and n(>xmin)>~300 is required to distinguish a log-normal distribution from a PL.\n",
    "    For more details, see figure 4.1 and section\n",
    "\n",
    "    **WARNING** This can take a very long time to run!  Execution time scales as \n",
    "    niter * setsize\n",
    "\n",
    "    \"\"\"\n",
    "    niter = int(niter)\n",
    "\n",
    "    ntail = sum(x >= xmin)\n",
    "    ntot = len(x)\n",
    "    nnot = ntot-ntail              # n(<xmin)\n",
    "    pnot = nnot/float(ntot)        # p(<xmin)\n",
    "    nonpldata = x[x<xmin]\n",
    "    nrandnot = sum( npr.rand(ntot) < pnot ) # randomly choose how many to sample from <xmin\n",
    "    nrandtail = ntot - nrandnot         # and the rest will be sampled from the powerlaw\n",
    "\n",
    "    ksv = []\n",
    "    if print_timing: deltat = []\n",
    "    for i in xrange(niter):\n",
    "        # first, randomly sample from power law\n",
    "        # with caveat!  \n",
    "        nonplind = numpy.floor(npr.rand(nrandnot)*nnot).astype('int')\n",
    "        fakenonpl = nonpldata[nonplind]\n",
    "        randarr = npr.rand(nrandtail)\n",
    "        fakepl = randarr**(1/(1-alpha)) * xmin \n",
    "        fakedata = numpy.concatenate([fakenonpl,fakepl])\n",
    "        if print_timing: t0 = time.time()\n",
    "        # second, fit to powerlaw\n",
    "        TEST = plfit(fakedata)\n",
    "        # Get the ks of the fakedata fitting\n",
    "        ksv.append(TEST[2])\n",
    "        # Print time information if needed\n",
    "        if print_timing: \n",
    "            deltat.append( time.time() - t0 )\n",
    "            print \"Iteration %i: %g seconds\" % (i, deltat[-1])\n",
    "    \n",
    "    ksv = numpy.array(ksv)\n",
    "    p = (ksv>ks).sum() / float(niter)\n",
    "\n",
    "    print \"p(%i) = %0.3f\" % (niter,p)\n",
    "    \n",
    "    if print_timing: print \"Iteration timing: %g +/- %g\" % (numpy.mean(deltat),numpy.std(deltat))\n",
    "\n",
    "    return p, ksv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all the code above and run the experiment. Perform the power-law fitting. Obtain its goodness-of-fit value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 6. WRITE YOUR CODE BELOW ##########################\n",
    "#Output xmin, a, ks of best fit, and p-value.\n",
    "\n",
    "\n",
    "print xmin\n",
    "print a\n",
    "print ks\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill out your answers here,  for the location of your choice. \n",
    "xmin = \n",
    "\n",
    "a = \n",
    "\n",
    "ks_value = \n",
    "\n",
    "p_value = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting: Plot the CDF of the input data and the fitted power-law distribution, in a log-log chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to plot the CDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotcdf(x, xmin, alpha, pointcolor='k', pointmarker='+', filename=''):\n",
    "    \"\"\"\n",
    "    Plots CDF and powerlaw\n",
    "    \"\"\"\n",
    "    # Generate CDF of x\n",
    "    x=numpy.sort(x)\n",
    "    n=len(x)\n",
    "    xcdf = numpy.arange(n,0,-1,dtype='float')/float(n)\n",
    "    \n",
    "    q = x[x>=xmin]\n",
    "    fcdf = (q/xmin)**(1-alpha)\n",
    "    nc = xcdf[argmax(x>=xmin)]\n",
    "    fcdf_norm = nc*fcdf\n",
    "\n",
    "    # Draw the fitted distribution as a line\n",
    "    D_location = argmax(xcdf[x>=xmin]-fcdf_norm)\n",
    "\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.loglog(x,xcdf,marker=pointmarker,color=pointcolor)\n",
    "    plt.loglog(q,fcdf_norm,'r')\n",
    "\n",
    "    if filename == '':\n",
    "        plt.show()\n",
    "    else:\n",
    "        # plt.show()\n",
    "        plt.savefig('3_'+filename) \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 7. WRITE YOUR CODE BELOW ##########################\n",
    "# Call plotcdf() to plot the distribution here\n",
    "# Remember to save your figure into file\n",
    "# If filename = '' or no filename provided, it will be not be saved.\n",
    "\n",
    "plotcdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the fit\n",
    "\n",
    "Do you think the fit is good? Explain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 (Extra Credit): Comparing with other models\n",
    "\n",
    "Besides power-law, there are several other popular distributions widely used, such as exponential $f(x)=e^{-\\lambda x}$, $C=\\lambda e^{\\lambda x_{min}}$, and log-normal distribution, etc.\n",
    "\n",
    "We suggest to use an public available powerlaw fitting python package, the 'powerlaw' (which can be found in either pip or Canopy package manager). Also install mpmath if you are encountering errors.\n",
    "\n",
    "The first task is to fit the data using exponential distribution, and compare with the power-law fitting by computing the likelihood ratio test. For more information about how to use powerlaw package, please see:\n",
    "\n",
    "http://nbviewer.ipython.org/gist/anonymous/bb4e1dfafd9e90d5bc3d\n",
    "\n",
    "http://pythonhosted.org/powerlaw/#powerlaw.Fit.distribution_compare\n",
    "\n",
    "###### Hints:\n",
    "1. In this experiment, you are first required to use fit.distribution_compare() function to compare fittings between power_law and lognormal distribution. The output will be which kind of distribution fits the data better\n",
    "2. Use plot_ccdf() function to draw the fitting. \n",
    "   * use fit.plot_ccdf() to plot the empiricial data distribution\n",
    "   * use fit.power_law.plot_ccdf() to plot the power_law fitting curve\n",
    "   * use fit.lognormal.plot_ccdf() to plot the log fitting curve\n",
    "3. Save your plots as figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################### 1x. WRITE YOUR CODE BELOW ##########################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "\n",
    "fit = powerlaw.Fit(x, discrete=False)\n",
    "# Using the functions \n",
    "# 1. fit.distribution_compare() to compare \n",
    "log_likelihood= \n",
    "print 'Log likelihood is '+ str(log_likelihood)\n",
    "\n",
    "# plot empirical CCDF, power law CCDF and log normal CCDFs\n",
    "fig = fit.\n",
    "fit.\n",
    "fit.\n",
    "\n",
    "fig.set_ylabel(r\"$p(X\\geq x)$\")\n",
    "fig.set_xlabel(r\"ITI\")\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig('4_compare_plots.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the plot you get above, which fit is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the log-likelihoods you get above, which fit is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Repeat the process for the data given in ./data/CU.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 1,000 tweets sent in Champaign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8. your code here (if needed). Or modify the above codes, and rerun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill out your answers here, for Champaign:\n",
    "x_min =\n",
    "\n",
    "a = \n",
    "\n",
    "ks_value = \n",
    " \n",
    "p_value = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the fit, for Champaign\n",
    "\n",
    "How does the fitted distribution compare to the original data?\n",
    "\n",
    "Do you think the fit is good? Explain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: ITI Fitting Using the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Twitter Crawler to collect 3,000 tweets choosing a specific location in the city list during the following week, and fit the ITI (Inter-Tweets Interval) distribution in the best model you tested in Exercise 2. (Tip: Choose your locations based on the pace of life!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions: \n",
    "Based on your experiments, answer:\n",
    "   * What's the main difference between the fittings between the data you collected (100 tweets), the 1,000 tweets, and the 3,000 tweets?\n",
    "   * And why?\n",
    "   * Is a low p-value more desirable than a high p-value? Explain in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your answers here \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
